{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.5-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python36564bitbasecondacd90c988128e4e428ceb9fe9a1123e59",
   "display_name": "Python 3.6.5 64-bit ('base': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IST 664 Final Project - SPAM Classification\n",
    "## Date: March 24, 2020\n",
    "## Team Members:\n",
    "* Thomas Bahng\n",
    "\n",
    "[https://github.com/tbahng/ist664-NLP-and-Classification](https://github.com/tbahng/ist664-NLP-and-Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import nltk\n",
    "from nltk import FreqDist, word_tokenize, bigrams\n",
    "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder, TrigramAssocMeasures, TrigramCollocationFinder\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from nltk.text import Text \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set(style = 'white')\n",
    "sns.set(style = 'whitegrid', color_codes = True)\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Help on class MultinomialNB in module sklearn.naive_bayes:\n\nclass MultinomialNB(BaseDiscreteNB)\n |  Naive Bayes classifier for multinomial models\n |  \n |  The multinomial Naive Bayes classifier is suitable for classification with\n |  discrete features (e.g., word counts for text classification). The\n |  multinomial distribution normally requires integer feature counts. However,\n |  in practice, fractional counts such as tf-idf may also work.\n |  \n |  Read more in the :ref:`User Guide <multinomial_naive_bayes>`.\n |  \n |  Parameters\n |  ----------\n |  alpha : float, optional (default=1.0)\n |      Additive (Laplace/Lidstone) smoothing parameter\n |      (0 for no smoothing).\n |  \n |  fit_prior : boolean, optional (default=True)\n |      Whether to learn class prior probabilities or not.\n |      If false, a uniform prior will be used.\n |  \n |  class_prior : array-like, size (n_classes,), optional (default=None)\n |      Prior probabilities of the classes. If specified the priors are not\n |      adjusted according to the data.\n |  \n |  Attributes\n |  ----------\n |  class_log_prior_ : array, shape (n_classes, )\n |      Smoothed empirical log probability for each class.\n |  \n |  intercept_ : array, shape (n_classes, )\n |      Mirrors ``class_log_prior_`` for interpreting MultinomialNB\n |      as a linear model.\n |  \n |  feature_log_prob_ : array, shape (n_classes, n_features)\n |      Empirical log probability of features\n |      given a class, ``P(x_i|y)``.\n |  \n |  coef_ : array, shape (n_classes, n_features)\n |      Mirrors ``feature_log_prob_`` for interpreting MultinomialNB\n |      as a linear model.\n |  \n |  class_count_ : array, shape (n_classes,)\n |      Number of samples encountered for each class during fitting. This\n |      value is weighted by the sample weight when provided.\n |  \n |  feature_count_ : array, shape (n_classes, n_features)\n |      Number of samples encountered for each (class, feature)\n |      during fitting. This value is weighted by the sample weight when\n |      provided.\n |  \n |  Examples\n |  --------\n |  >>> import numpy as np\n |  >>> X = np.random.randint(5, size=(6, 100))\n |  >>> y = np.array([1, 2, 3, 4, 5, 6])\n |  >>> from sklearn.naive_bayes import MultinomialNB\n |  >>> clf = MultinomialNB()\n |  >>> clf.fit(X, y)\n |  MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n |  >>> print(clf.predict(X[2:3]))\n |  [3]\n |  \n |  Notes\n |  -----\n |  For the rationale behind the names `coef_` and `intercept_`, i.e.\n |  naive Bayes as a linear classifier, see J. Rennie et al. (2003),\n |  Tackling the poor assumptions of naive Bayes text classifiers, ICML.\n |  \n |  References\n |  ----------\n |  C.D. Manning, P. Raghavan and H. Schuetze (2008). Introduction to\n |  Information Retrieval. Cambridge University Press, pp. 234-265.\n |  https://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html\n |  \n |  Method resolution order:\n |      MultinomialNB\n |      BaseDiscreteNB\n |      BaseNB\n |      sklearn.base.BaseEstimator\n |      sklearn.base.ClassifierMixin\n |      builtins.object\n |  \n |  Methods defined here:\n |  \n |  __init__(self, alpha=1.0, fit_prior=True, class_prior=None)\n |      Initialize self.  See help(type(self)) for accurate signature.\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes defined here:\n |  \n |  __abstractmethods__ = frozenset()\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from BaseDiscreteNB:\n |  \n |  fit(self, X, y, sample_weight=None)\n |      Fit Naive Bayes classifier according to X, y\n |      \n |      Parameters\n |      ----------\n |      X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n |          Training vectors, where n_samples is the number of samples and\n |          n_features is the number of features.\n |      \n |      y : array-like, shape = [n_samples]\n |          Target values.\n |      \n |      sample_weight : array-like, shape = [n_samples], (default=None)\n |          Weights applied to individual samples (1. for unweighted).\n |      \n |      Returns\n |      -------\n |      self : object\n |  \n |  partial_fit(self, X, y, classes=None, sample_weight=None)\n |      Incremental fit on a batch of samples.\n |      \n |      This method is expected to be called several times consecutively\n |      on different chunks of a dataset so as to implement out-of-core\n |      or online learning.\n |      \n |      This is especially useful when the whole dataset is too big to fit in\n |      memory at once.\n |      \n |      This method has some performance overhead hence it is better to call\n |      partial_fit on chunks of data that are as large as possible\n |      (as long as fitting in the memory budget) to hide the overhead.\n |      \n |      Parameters\n |      ----------\n |      X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n |          Training vectors, where n_samples is the number of samples and\n |          n_features is the number of features.\n |      \n |      y : array-like, shape = [n_samples]\n |          Target values.\n |      \n |      classes : array-like, shape = [n_classes] (default=None)\n |          List of all the classes that can possibly appear in the y vector.\n |      \n |          Must be provided at the first call to partial_fit, can be omitted\n |          in subsequent calls.\n |      \n |      sample_weight : array-like, shape = [n_samples] (default=None)\n |          Weights applied to individual samples (1. for unweighted).\n |      \n |      Returns\n |      -------\n |      self : object\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors inherited from BaseDiscreteNB:\n |  \n |  coef_\n |  \n |  intercept_\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from BaseNB:\n |  \n |  predict(self, X)\n |      Perform classification on an array of test vectors X.\n |      \n |      Parameters\n |      ----------\n |      X : array-like, shape = [n_samples, n_features]\n |      \n |      Returns\n |      -------\n |      C : array, shape = [n_samples]\n |          Predicted target values for X\n |  \n |  predict_log_proba(self, X)\n |      Return log-probability estimates for the test vector X.\n |      \n |      Parameters\n |      ----------\n |      X : array-like, shape = [n_samples, n_features]\n |      \n |      Returns\n |      -------\n |      C : array-like, shape = [n_samples, n_classes]\n |          Returns the log-probability of the samples for each class in\n |          the model. The columns correspond to the classes in sorted\n |          order, as they appear in the attribute `classes_`.\n |  \n |  predict_proba(self, X)\n |      Return probability estimates for the test vector X.\n |      \n |      Parameters\n |      ----------\n |      X : array-like, shape = [n_samples, n_features]\n |      \n |      Returns\n |      -------\n |      C : array-like, shape = [n_samples, n_classes]\n |          Returns the probability of the samples for each class in\n |          the model. The columns correspond to the classes in sorted\n |          order, as they appear in the attribute `classes_`.\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from sklearn.base.BaseEstimator:\n |  \n |  __getstate__(self)\n |  \n |  __repr__(self, N_CHAR_MAX=700)\n |      Return repr(self).\n |  \n |  __setstate__(self, state)\n |  \n |  get_params(self, deep=True)\n |      Get parameters for this estimator.\n |      \n |      Parameters\n |      ----------\n |      deep : boolean, optional\n |          If True, will return the parameters for this estimator and\n |          contained subobjects that are estimators.\n |      \n |      Returns\n |      -------\n |      params : mapping of string to any\n |          Parameter names mapped to their values.\n |  \n |  set_params(self, **params)\n |      Set the parameters of this estimator.\n |      \n |      The method works on simple estimators as well as on nested objects\n |      (such as pipelines). The latter have parameters of the form\n |      ``<component>__<parameter>`` so that it's possible to update each\n |      component of a nested object.\n |      \n |      Returns\n |      -------\n |      self\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors inherited from sklearn.base.BaseEstimator:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  __weakref__\n |      list of weak references to the object (if defined)\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from sklearn.base.ClassifierMixin:\n |  \n |  score(self, X, y, sample_weight=None)\n |      Returns the mean accuracy on the given test data and labels.\n |      \n |      In multi-label classification, this is the subset accuracy\n |      which is a harsh metric since you require for each sample that\n |      each label set be correctly predicted.\n |      \n |      Parameters\n |      ----------\n |      X : array-like, shape = (n_samples, n_features)\n |          Test samples.\n |      \n |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n |          True labels for X.\n |      \n |      sample_weight : array-like, shape = [n_samples], optional\n |          Sample weights.\n |      \n |      Returns\n |      -------\n |      score : float\n |          Mean accuracy of self.predict(X) wrt. y.\n\n"
    }
   ],
   "source": [
    "help(MultinomialNB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Lorum ipsum...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "Lorum ipsum...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "Lorum ipsum...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration\n",
    "Lorum ipsum...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "Lorum ipsum...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "Lorum ipsum...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Lorum ipsum...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}